[["index.html", "Statistical Rethinking - Notes About", " Statistical Rethinking - Notes Hamza 2024-05-21 About My study notes and solutions for exercises from the Statistical Rethinking book "],["the-golem-of-prague.html", "1. The Golem of Prague", " 1. The Golem of Prague TODO "],["small-worlds-and-large-worlds.html", "2. Small Worlds and Large Worlds Example (p. 33) Grid Approximation Quadratic Approximation Markov Chain Monte Carlo (MCMC)", " 2. Small Worlds and Large Worlds Example (p. 33) The likelihood of the data: 6 W in 9 tosses, under the probability of 0.5 dbinom(6, size=9, prob=0.5) ## [1] 0.1640625 Grid Approximation For the globe tossing example, lets make a grid of 20 points # change this for more precision. however, there will not be much change in inference after 100 points num_of_points = 20 # 1. define grid p_grid &lt;- seq(from=0, to=1, length.out=num_of_points) # 2. define prior prior &lt;- rep(1, num_of_points) # 3. compute likelihood at each value in the grid likelihood &lt;- dbinom(6, size=9, prob=p_grid) # 4. compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # 5. standarize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) Lets display the posterior distribution: plot(p_grid, posterior, type=&quot;b&quot;, xlab = &quot;prob. of water&quot;, ylab=&quot;posterior prob.&quot;) Different prior #1: num_of_points = 20 # 1. define grid p_grid &lt;- seq(from=0, to=1, length.out=num_of_points) # 2. define prior prior &lt;- ifelse(p_grid &lt; 0.5, 0, 1) # 3. compute likelihood at each value in the grid likelihood &lt;- dbinom(6, size=9, prob=p_grid) # 4. compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # 5. standarize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) # 6. plot plot(p_grid, posterior, type=&quot;b&quot;, xlab = &quot;prob. of water&quot;, ylab=&quot;posterior prob.&quot;) Different prior #2: num_of_points = 20 # 1. define grid p_grid &lt;- seq(from=0, to=1, length.out=num_of_points) # 2. define prior prior &lt;- exp(-5*abs(p_grid - 0.5)) # 3. compute likelihood at each value in the grid likelihood &lt;- dbinom(6, size=9, prob=p_grid) # 4. compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # 5. standarize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) # 6. plot plot(p_grid, posterior, type=&quot;b&quot;, xlab = &quot;prob. of water&quot;, ylab=&quot;posterior prob.&quot;) Quadratic Approximation a.k.a. Gaussian Approximation, because we assume that the posterior is Gaussian library(rethinking) globe.qa &lt;- quap( alist( W ~ dbinom(W+L, p), # binomial likelihood p ~ dunif(0, 1) # uniform prior ), data=list(W=6, L=3) ) # print a summary of quadratic approx. precis(globe.qa) ## mean sd 5.5% 94.5% ## p 0.666667 0.1571337 0.4155369 0.917797 The output refers to the properties of the posterior, assuming it is Gaussian. Lets compare this approximation with the real posterior distribution (it has a Beta distribution analytically) W &lt;- 6 L &lt;- 3 # from the previous outputs globe.qa.mean &lt;- 0.67 globe.qa.sd &lt;- 0.16 # the real posterior distribution curve(dbeta(x, W+1, L+1), from=0, to=1) # the quadratic/gaussian approximation curve(dnorm(x, globe.qa.mean, globe.qa.sd), lty=2, add=TRUE) Markov Chain Monte Carlo (MCMC) n_samples &lt;- 1000 p &lt;- rep(NA, n_samples) p[1] &lt;- 0.5 W &lt;- 6 L &lt;- 3 for (i in 2:n_samples) { p_new &lt;- rnorm(1, p[i-1], 0.1) if (p_new &lt; 0) p_new &lt;- ans(p_new) if (p_new &gt; 1) p_new &lt;- 2 - p_new q0 &lt;- dbinom(W, W+L, p[i-1]) q1 &lt;- dbinom(W, W+L, p_new) p[i] &lt;- ifelse(runif(1) &lt; q1/q0, p_new, p[i-1]) } # compare the MCMC approximation with the analytical posterior (beta) dens(p, xlim=c(0,1)) curve(dbeta(x, W+1, L+1), lty=2, add=TRUE) "],["sampling-the-imaginary.html", "3. Sampling the Imaginary Sampling from a grid-approximate posterior Sampling to summarize Sampling to simulate prediction", " 3. Sampling the Imaginary Instead of relying on calculus to compute the posterior, we can use the sampling method. This chapter is about: How to draw a sample Summarizing the posterior using the sample Boundaries Probability mass Point Estimate Simulation using the sample Sampling from a grid-approximate posterior Below is the code for computing the posterior for the globe tossing model using grid approximation from chapter 2 p_grid &lt;- seq(from=0, to=1, length.out=1000) prior &lt;- rep(1, 1000) likelihood &lt;- dbinom(6, size=9, prob=p_grid) posterior &lt;- likelihood * prior posterior &lt;- posterior / sum(posterior) Now, to draw a 10,000 samples from the posterior, we do this samples &lt;- sample(p_grid, prob=posterior, size=1e4, replace=TRUE) The resulting samples are shown in this plot: plot(samples) And the following is density estimate computed from these samples: library(rethinking) dens(samples) Comparing with the density of the posterior computed via grid approxinmation, we found that it is very similar though it isnt identical: plot(p_grid, posterior, type=&quot;b&quot;, xlab = &quot;prob. of water&quot;, ylab=&quot;posterior prob.&quot;) Sampling to summarize We prepared the model in the previous section. Now, we cam use it to summarize and interpret the posterior distribution. This is done by asking the model quesions about the following: Intervals of defined boundaries What is the probability (i.e. posterior probability) that the proportion of water is less than 0.5? Using grid approx: sum(posterior[p_grid &lt; 0.5]) ## [1] 0.1718746 Using samples from posterior sum(samples &lt; 0.5)/length(samples) ## [1] 0.1738 We can see that the results are very close to each other. How much does posterior probability lie between 0.5 and 0.75? sum(samples &gt; 0.5 &amp; samples &lt; 0.75) / length(samples) ## [1] 0.6044 This means that about 60% of the posterior probability lies between 0.5 and 0.75. Intervals of defined mass Compatibility Interval It is usually called: Confidence Interval in Frequentist stats Credible Interval in Bayesian stats However, the author calls it Compatibility Interval because: It indicates a range of parameter values compatible with the model and data. He doesnt use the confidence term because the model, data, and interval may not inspire confidence What is the boundaries of parameter values (i.e. possible proportions of water) that holds the lower 80% posterior probability? quantile(samples, 0.8) ## 80% ## 0.7597598 The output 0.76 represents the stop point of the interval. So, the interval or parameters [0, 0.76] holds 80% of the posterior probability, i.e. the 80th percentile lies in it. The boundaries of the middle 80% posterior probability {r} quantile(samples, c(0.1, 0.9))} Percentile Intervals (PI) Assign equal probability mass to each tail. Common in scientific literature Good for summarizing the shape of distribution as long as it is not too asymmetrical p_grid &lt;- seq(from=0, to=1, length.out=1000) prior &lt;- rep(1, 1000) # observing 3 waters in 3 tosses likelihood &lt;- dbinom(3, size=3, prob=p_grid) posterior &lt;- likelihood * prior posterior &lt;- posterior / sum(posterior) samples &lt;- sample(p_grid, size=1e4, replace=TRUE, prob=posterior) dens(samples) PI(samples, prob=0.5) ## 25% 75% ## 0.7037037 0.9319319 The last line compute the PI assigning 25% of the probability mass to each end of the interval. Highest Posterior Density Interval (HPDI) HPDI: the narrowest interval containing the specified probability mass. It can be computed using this function with (prob=0.5) as probability mass: HPDI(samples, prob=0.5) ## |0.5 0.5| ## 0.8368368 0.9979980 This means that the interval between the parameter values (i.e. proportion of water in our case) 0.56 and 0.75 has the highest posterior probability Notes: Most of the time, PI and HPDI are very similar except for the skewed distributions. It doesnt matter which type of interval to use in bell shape curves. If choice of interval type makes a big difference, then we shouldnt be using them to summarize the posterior. PLOT THE ENTIRE POSTERIOR INSTEAD! Point Estimates In Bayesian stats, parameter estimate = the entire posterior distribution != single number = function: Parameter value -&gt; Posterior distribution (function) -&gt; Plausibility value Why? Because this way we avoid discarding information about uncertainty in the entire posterior distribution. However, what if we want to produce a single point estimate to describe the posterior? Here are some common choices for doing that. Note that using single parameter value for making inference/prediction leads to overconfidence, so make sure to use the posterior. Maximum a Posteriori Estimate (MAP) From the grid approximation: p_grid[which.max(posterior)] ## [1] 1 From the sample (its called the mode or MAP): chainmode(samples, adj=0.01) ## [1] 0.9728613 Loss Function Loss function is helpful to decide a single point estimate, here is how we do so: First, we must pick a loss function suitable to the problem. Then, we find the value that minimize the loss to use it as a single point estimate, i.e. the optimal point estimate Common loss functions: Absolute loss \\(| decision - true \\space value |\\) -&gt; median of the posterior is the optimal point estimate Quadratic loss \\((decision - true \\space value)^2\\) -&gt; mean of the posterior is the optimal point estimate Note: when the posterior distribution is symmetrical and normal looking = then the median and mean converge to the same point (i.e. it doesnt matter which loss or point estimate to pick) Example: Median median(samples) ## [1] 0.8388388 The expected loss when we decide that the proportion of water p=0.5 = sum of the weighted average loss: sum(posterior * abs(0.5 - p_grid)) ## [1] 0.3128752 We can find the loss for every possible decision/value/proportion of water in p_grid: loss &lt;- sapply(p_grid, function(d) sum(posterior * abs(d - p_grid))) After that, we can find the parameter/decision that minimizes the loss: p_grid[which.min(loss)] ## [1] 0.8408408 And this is actually the posterior median Final Notes on Summarization Usually, it is better to communicate as much as you can about: Posterior distribution Data and Model so that others can build upon your work (Scientists thinking vs. Statistician thinking!) Sampling to simulate prediction Dummy Data Likelihood functions work in both directions: Given data, find how plausible it is: for Binomial, we use dbinom Given the distribution and its parameters, simulate data (by sampling): for Binomial, we use rbinom Either way, Bayesian models are always generative, generate data through simulation or parameters through estimation. Lets see in practice: Using the true proportion of water on Earth prob=0.7, lets find probability of observing 0, 1, or 2 water in 2 tosses: dbinom(0:2, size=2, prob = 0.7) ## [1] 0.09 0.42 0.49 Lets generate n=10 simulations/dummy observations with the same distribution properties. Remember, running a single simulation means tossing the earth size=2 times with prob=0.7 of observing water: rbinom(n=10, size=2, prob=0.7) ## [1] 1 2 1 1 1 1 2 2 1 2 Lets generate 100,000 dummy data (i.e. water observation) to verify that each value (0, 1, 2) appears in proportion to its likelihood: dummy_water &lt;- rbinom(n=1e5, size=2, prob=0.7) table(dummy_water)/1e5 ## dummy_water ## 0 1 2 ## 0.09086 0.41931 0.48983 Very close to the computed likelihood. The difference is called the simulation variance and it is changed every execution. dummy_water &lt;- rbinom(n=1e5, size=100, prob=0.7) simplehist(dummy_water, xlab=&quot;dummy water count&quot;) Model Checking # simulate for a single value of p w &lt;- rbinom(1e4, size=9, prob=0.6) # propogate parameter uncertainty into predictions w &lt;- rbinom(1e4, size=9, prob=samples) simplehist(w) "],["geocentric-models.html", "4. Geocentric Models Normal Distribution Gaussian model of human height Linear prediction Curves from lines", " 4. Geocentric Models Normal Distribution Normal by Addition (Keep in mind the example of field soccer, coin tossing, and stepping right and left on page 72) library(rethinking) # given that the steps for each person is represented by a list of 16 random numbers # between -1 and 1: # run 1000 simulation of stepping left and right, and store the final result/position pos &lt;- replicate(1000, sum(runif(16,-1,1))) # plot the end positions around the half line of soccer field plot(pos) # plot the density of the positions plot(density(pos)) Normal by multiplication (See the example on page 74) Note that the interaction between the growth deviations converges to Gaussian dist as long as the effect is small. growth_samll_effect &lt;- replicate(1000, prod(1+runif(12,0,0.1))) dens(growth_samll_effect, norm.comp = TRUE) growth_big_effect &lt;- replicate(10000, prod(1+runif(12,0,0.5))) dens(growth_big_effect, norm.comp = TRUE) Normal by log-multiplication Multiplication interactions of large deviations converges to Gaussian dist when we measure the outcomes on the log scale. log.big &lt;- replicate(10000, log(prod(1+runif(12,0,0.5)))) dens(log.big, norm.comp = TRUE) Gaussian model of human height The data library(rethinking) data(&quot;Howell1&quot;) d &lt;- Howell1 # explore data str(d) ## &#39;data.frame&#39;: 544 obs. of 4 variables: ## $ height: num 152 140 137 157 145 ... ## $ weight: num 47.8 36.5 31.9 53 41.3 ... ## $ age : num 63 63 65 41 51 35 32 27 19 54 ... ## $ male : int 1 0 0 1 0 1 0 1 0 1 ... # data summary precis(d, hist=FALSE) ## mean sd 5.5% 94.5% ## height 138.2635963 27.6024476 81.108550 165.73500 ## weight 35.6106176 14.7191782 9.360721 54.50289 ## age 29.3443934 20.7468882 1.000000 66.13500 ## male 0.4724265 0.4996986 0.000000 1.00000 The model and prior Based on domain-specific information, we decide that the range of plausible human heights is \\(178 \\mp 40\\). The std. deviation must be basically positive \\(h_i \\sim Normal(\\mu, \\sigma)\\) Given that the parameters are independent, the prior is: \\(Pr(\\mu, \\sigma) = Pr(\\mu)Pr(\\sigma)\\) Where: \\(\\mu \\sim Normal(178,20)\\) \\(\\sigma \\sim Uniform(0, 50)\\) Lets plot the priors: - Mean curve(dnorm(x, 178, 20), from=100, to=250) Std. deviation curve(dunif(x, 0, 50), from=0, to=60) Lets simulate heights based on the priors. This is called the prior predictive simulation: sample_mu &lt;- rnorm(1e4, 178, 20) sample_sigma &lt;- runif(1e4, 0, 50) prior_h &lt;- rnorm(1e4, sample_mu, sample_sigma) dens(prior_h) So far, the model is defined before showing it the data. We can change the prior \\(\\mu\\) std. deviation to see how the model is sensitive to the prior choices that arent relying on scintific knowledge as we did. sample_mu &lt;- rnorm(1e4, 178, 100) sample_sigma &lt;- runif(1e4, 0, 50) prior_h &lt;- rnorm(1e4, sample_mu, sample_sigma) dens(prior_h) Note how the result doesnt make sense with negative and very large heights. Grid approximation of the posterior distribution Since we have 2 parameters, grid approx. method is not practical. However, we will try using it computing the log-likelihood: # we will compute the approximation using the height data of persons over 18 y.o. d2 &lt;- d[d$age &gt;= 18,] mu.list &lt;- seq(from=150, to=160, length.out=100) sigma.list &lt;- seq(from=7, to=9, length.out=100) # Create a Data Frame from All Combinations of Factor Variables post &lt;- expand.grid(mu=mu.list, sigma=sigma.list) # compute the log-likelihood post$LL &lt;- sapply( 1:nrow(post), function(i) sum ( dnorm(d2$height, post$mu[i], post$sigma[i], log=TRUE) ) ) post$prod &lt;- post$LL + dnorm(post$mu, 178, 20, TRUE) + dunif(post$sigma, 0, 50, TRUE) post$prob &lt;- exp(post$prod - max(post$prod)) contour_xyz(post$mu, post$sigma, post$prob) image_xyz(post$mu, post$sigma, post$prob) Sampling from the posterior # generate random indexes of rows sample.rows &lt;- sample(1:nrow(post), size=1e4, replace=TRUE, prob=post$prob) sample.mu &lt;- post$mu[sample.rows] sample.sigma &lt;- post$sigma[sample.rows] # this shows the most plausible combinations of mu and sigma plot(sample.mu, sample.sigma, cex=0.5, pch=16, col=col.alpha(rangi2, 0.1)) Lets check the shape of marginal posterior densities: dens(sample.mu) dens(sample.sigma) Note that the density for sigma has a longer right tail d3 &lt;- sample(d2$height, size = 20) mu.list &lt;- seq( from=150, to=170 , length.out=200 ) sigma.list &lt;- seq( from=4 , to=20 , length.out=200 ) post2 &lt;- expand.grid( mu=mu.list , sigma=sigma.list ) post2$LL &lt;- sapply( 1:nrow(post2) , function(i) sum( dnorm( d3 , mean=post2$mu[i] , sd=post2$sigma[i] , log=TRUE ) ) ) post2$prod &lt;- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) + dunif( post2$sigma , 0 , 50 , TRUE ) post2$prob &lt;- exp( post2$prod - max(post2$prod) ) sample2.rows &lt;- sample( 1:nrow(post2) , size=1e4 , replace=TRUE , prob=post2$prob ) sample2.mu &lt;- post2$mu[ sample2.rows ] sample2.sigma &lt;- post2$sigma[ sample2.rows ] plot( sample2.mu , sample2.sigma , cex=0.5 , col=col.alpha(rangi2,0.1) , xlab=&quot;mu&quot; , ylab=&quot;sigma&quot; , pch=16 ) dens(sample2.sigma, norm.comp = TRUE) Finding the posterior with quap Quadratic Approximation is good to make inferences about the shape of posterior, particularly its peak that lie at the maximum a posteriori (MAP) Lets first load the data: # load data library(rethinking) data(Howell1) d &lt;- Howell1 d2 &lt;- d[ d$age &gt;= 18 , ] Now, we will define our model with code: \\(h_i \\sim Normal(\\mu, \\sigma)\\) \\(\\mu \\sim Normal(178,20)\\) \\(\\sigma \\sim Uniform(0, 50)\\) flist &lt;- alist( height ~ dnorm(mu, sigma), mu ~ dnorm(178, 20), sigma ~ dunif(0, 50) ) Note: alist stores formulas without executing the expression in the code unlike list Now, we fit the mode to the data in `d2`: m4.1 &lt;- quap(flist, data=d2) Lets take a glance at the model (i.e posterior dist): precis(m4.1) ## mean sd 5.5% 94.5% ## mu 154.607201 0.4119492 153.948827 155.265575 ## sigma 7.730478 0.2913055 7.264916 8.196041 Sampling from a quap post &lt;- extract.samples(m4.1, n=1e4) head(post) ## mu sigma ## 1 155.3808 7.372322 ## 2 154.8912 7.774284 ## 3 155.1147 7.671096 ## 4 154.1441 7.818519 ## 5 155.0759 7.983457 ## 6 154.8982 7.093661 precis(post, hist=FALSE) ## mean sd 5.5% 94.5% ## mu 154.604963 0.4134325 153.952468 155.276243 ## sigma 7.732341 0.2863066 7.282393 8.195132 Comparing these values to the output from precis(m4.1), we found it very close. plot(post) Sampling the multivariate posterior w/o rethinking The function extract.samples runs the following simulation that samples random vectors of multivariate Gaussian values. This simulation requires computing the variance-covariance matrix library(MASS) post &lt;- mvrnorm(n=1e4, mu=coef(m4.1), Sigma=vcov(m4.1)) plot(post) Variance-Covariance Matrix It is an essential compnent in the quap algorithm. It tells us how each parameter relates to every other parameter in the posterior distribution. It can be factored into 2 elements: Vector of variances for the parameters diag(vcov(model)) Correlation matrix that tells how changes in one parameter lead to correlated changes in the others cov2cor(vcov(model)) Linear prediction Using the association between predictor variables and outcome variable, we want to predict the later. This is how linear regression works. Linear model strategy: probabilistic approach We tell the model (golem) the following: Assume that the predictor variable has a constant and additive relationship to the mean of the outcome. Consider all the lines (formed by the combinations of parameter values) that relate one variable (or more) to the other. Rank all of these lines by plausibility, given these data. The resulted model is a posterior distribution In the following example, we want to predict the height using the weight as a predictor variable. This code plot the data to use in model fitting: library(rethinking) data(Howell1) d &lt;- Howell1 d2 &lt;- d[d$age &gt;= 18,] plot(d2$height ~ d2$weight) We want to use the Gaussian model of height we built in the previous chapters but making the mean of height \\(\\mu_i\\) is a function of weights where weight values are denoted by \\(x\\). Here is the model: \\(h_i \\sim Normal(\\mu_i, \\sigma)\\) \\(\\mu_i = \\alpha + \\beta(x_i - \\bar{x})\\) \\(\\alpha \\sim Normal(178, 20)\\) \\(\\beta \\sim Normal(0, 10)\\) \\(\\sigma \\sim Uniform(0,50)\\) Notations: \\(\\bar{x}\\) is the mean of weights \\(x_i\\) weight at row \\(i\\) \\(\\mu_i\\) the mean of heights are row \\(i\\) \\(h_i\\) the height at row \\(i\\) \\(\\alpha, \\beta\\) are parameters to learn Note all relationships are Stochastic except the relationship between the height mean and weight. The parameters are made up as devices that will help us to manipulate \\(\\mu\\). Here is what each parameter does: \\(\\alpha\\) (intercept): represents the expected height when \\(x_i=\\bar{x}\\) \\(\\beta\\) (slope): represents the rate of change in expectation when \\(x_i\\) changes by 1 unit Priors The unobserved variables are called parameters (\\(\\alpha, \\beta, \\sigma\\)) and their distributions are called priors. Each combination of parameter values implies a unique line Lets simulate the prior predictive distribution to see the possible lines set.seed(2971) N &lt;- 100 # 100 lines a &lt;- rnorm(N, 178, 20) b &lt;- rnorm(N, 0, 10) # prepare the canvas for plotting plot(NULL, xlim=range(d2$weight), ylim=c(-100,400), xlab=&quot;weight&quot;, ylab=&quot;height&quot;) abline(h=0, lty=2) # no one is shorter than zero! abline(h=272, lty=1, lwd=0.5) # the world&#39;s tallest person xbar &lt;- mean(d2$weight) # simulate the possible lines for (i in 1:N) curve(a[i] + b[i]*(x-xbar), from=min(d2$weight), to=max(d2$weight), add=TRUE, col=col.alpha(&quot;black&quot;, 0.2)) As we can see, not all the lines seem to represent the relationship between weight and height for human. Negative relationship doesnt make sense in this context. We want to restrict \\(\\beta\\) to positive numbers so we only get positive relationship. Therefore, we can define the prior as Log-Normal instead to enforce positive relationship: \\[ \\beta \\sim Log-Normal(0,1) \\] b &lt;- rlnorm(1e4, 0, 1) dens(b, xlim=c(-1,5), adj=0.1) We can see the distribution is defined only on the positive beta values. Now, lets do the prior predictive simulation again with the new prior: set.seed(2971) N &lt;- 100 # 100 lines a &lt;- rnorm(N, 178, 20) b &lt;- rlnorm(N, 0, 1) # log-normal prior # prepare the canvas for plotting plot(NULL, xlim=range(d2$weight), ylim=c(-100,400), xlab=&quot;weight&quot;, ylab=&quot;height&quot;) abline(h=0, lty=2) # no one is shorter than zero! abline(h=272, lty=1, lwd=0.5) # the world&#39;s tallest person xbar &lt;- mean(d2$weight) # simulate the possible lines for (i in 1:N) curve(a[i] + b[i]*(x-xbar), from=min(d2$weight), to=max(d2$weight), add=TRUE, col=col.alpha(&quot;black&quot;, 0.2)) Now, the result is much more sensible! Finding the posterior distribution The model is defined now along with the priors. We are now ready to build the posterior approximation using quap library(rethinking) data(&quot;Howell1&quot;) d &lt;- Howell1 d2 &lt;- d[d$age &gt;= 18,] xbar &lt;- mean(d2$weight) # fit the model m4.3 &lt;- quap( alist( height ~ dnorm(mu, sigma), mu &lt;- a + b * (weight-xbar), a ~ dnorm(178, 20), b ~ dlnorm(0, 1), sigma ~ dunif(0, 50) ), data=d2 ) To interpret the posterior, we can use either tables or plots. Plots gives more information about the posterior. However, lets see the summary table: precis(m4.3) ## mean sd 5.5% 94.5% ## a 154.6013671 0.27030766 154.1693633 155.0333710 ## b 0.9032807 0.04192363 0.8362787 0.9702828 ## sigma 5.0718809 0.19115478 4.7663786 5.3773831 We also need to see the covariance among the parameters by computing the variance-covariance matrix: round(vcov(m4.3), 3) ## a b sigma ## a 0.073 0.000 0.000 ## b 0.000 0.002 0.000 ## sigma 0.000 0.000 0.037 Plotting the posterior against the data plot(height~weight, data=d2, col=rangi2) post &lt;- extract.samples(m4.3) a_map &lt;- mean(post$a) b_map &lt;- mean(post$b) curve(a_map + b_map * (x-xbar), add=TRUE) post[1:5,] ## a b sigma ## 1 154.5564 0.9000245 4.768499 ## 2 154.4602 0.8474034 5.136055 ## 3 154.7858 0.9440438 5.313512 ## 4 154.4490 0.8806246 4.683555 ## 5 154.4639 0.8397176 5.185017 Uncertainty around the mean We want to know the uncertainty around the mean of posterior in order to determine the confidence in the relationship between predictor and outcome, since the posterior we plot in the previous step is the MAP, which is the mean of many lines formed by the posterior. Here is a sample of possible lines: post[1:5,] ## a b sigma ## 1 154.5564 0.9000245 4.768499 ## 2 154.4602 0.8474034 5.136055 ## 3 154.7858 0.9440438 5.313512 ## 4 154.4490 0.8806246 4.683555 ## 5 154.4639 0.8397176 5.185017 Lets see how the confident about the location of the mean changes based on data size. First, we will extract the first 10 cases and re-estimate the model: N &lt;- 10 dN &lt;- d2[1:N, ] mN &lt;- quap( alist( height ~ dnorm(mu, sigma), mu &lt;- a+b*(weight - mean(weight)), a ~ dnorm(178, 20), b ~ dlnorm(0, 1), sigma ~ dunif(0, 50) ), data=dN ) Plot 20 of these lines to see what the uncertainty looks like: post &lt;- extract.samples(mN, n=20) # plot the 10 sampled cases plot(dN$weight, dN$height, xlim=range(d2$weight), ylim=range(d2$height), col=rangi2, xlab=&quot;weight&quot;, ylab=&quot;height&quot;) mtext(concat(&quot;N = &quot;, N)) for(i in 1:20) curve(post$a[i] + post$b[i] * (x-mean(dN$weight)), col=col.alpha(&quot;black&quot;, 0.3), add=TRUE) Plotting regression intervals and contours Lets find the quadratic posterior distribution of the mean height \\(\\mu\\) when weight is 50 kg. This distribution represents the relative plausibility of different values of the mean: post &lt;- extract.samples(m4.3) mu_at_50 &lt;- post$a + post$b * (50-xbar) dens(mu_at_50, col=rangi2, lwd=2, xlab=&quot;mu|weight=50&quot;) Compatibility interval of \\(\\mu\\) at 50 kg is: PI(mu_at_50, prob = 0.89) ## 5% 94% ## 158.5839 159.6832 To do that for all weight values: mu &lt;- link(m4.3) str(mu) ## num [1:1000, 1:352] 157 157 157 158 157 ... The resulted matrix contains 352 columns, each corresponds to one row in the d2 data. It contains 1000 rows, each represents a sample. Therefore, the matrix contains a distribution of \\(\\mu\\) for each individual in the original data d2. Lets plot the Gaussian distribution for each mean value: plot(height ~ weight, d2, type=&quot;n&quot;) for(i in 1:100) points(d2$weight, mu[i,], pch=16, col=col.alpha(rangi2, 0.1)) The pile of points represents the rows. The plot is kind of missy, lets do that for a small group of weight values weight.seq &lt;- seq(from=25, to=70, by=1) mu &lt;- link(m4.3, data=data.frame(weight=weight.seq)) plot(height ~ weight, d2, type=&quot;n&quot;) for(i in 1:100) points(weight.seq, mu[i,], pch=16, col=col.alpha(rangi2, 0.1)) Now, lets summarize the distribution of mu # compute the mean of each column (dimension 2) of the matrix mu mu.mean &lt;- apply(mu, 2, mean) mu.PI &lt;- apply(mu, 2, PI, prob=0.89) # plot the line and the interval plot(height ~ weight, data=d2, col=col.alpha(rangi2, 0.5)) lines(weight.seq, mu.mean) shade(mu.PI, weight.seq) How link works This approach can be used to generate posterior predictions for any component of any model post &lt;- extract.samples(m4.3) mu.link &lt;- function(weight) post$a + post$b*(weight-xbar) weight.seq &lt;- seq(25,70,1) mu &lt;- sapply(weight.seq, mu.link) mu.mean &lt;- apply(mu, 2, mean) mu.PI &lt;- apply(mu, 2, PI, prob=0.89) Summary: recipe of generating predictions and intervals from the posterior Use link to generate distributions posterior values for \\(\\mu\\) Use mean or PI to find averages and bounds of \\(\\mu\\) for each value of the predictor variable Plot the lines and intervals using lines and shades or the distribution of the prediction given the value of predictor(s) Prediction intervals What weve done so far is just use samples from the posterior to visualize the uncertainty in \\(\\mu_i\\). Now, we want to compute the predictions of heights thats distributed according to: \\(h_i \\sim Normal(\\mu_i, \\sigma)\\) Lets simulate heights: # simulate 1e3 data by default sim.height &lt;- sim(m4.3, data=list(weight=weight.seq)) str(sim.height) ## num [1:1000, 1:46] 146 139 136 135 138 ... The resulted matrix contains 1000 simulated heights (rows) for 46 weight values (columns). Lets summarize it: height.PI &lt;- apply(sim.height, 2, PI, prob=0.89) height.PI ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## 5% 128.1853 129.4536 130.1615 131.0184 131.5920 132.3106 133.6148 ## 94% 144.4269 145.3213 146.0324 147.5608 148.3459 149.0710 150.5475 ## [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## 5% 135.5421 135.8942 136.3520 137.6058 138.6176 139.6518 140.1450 ## 94% 151.3630 151.7098 152.9104 153.9036 154.4739 155.3530 156.1087 ## [,15] [,16] [,17] [,18] [,19] [,20] [,21] ## 5% 140.8575 141.6930 143.4392 143.9476 144.7856 145.0695 146.4285 ## 94% 157.2042 158.1098 158.6873 159.7029 160.9985 161.8457 162.9799 ## [,22] [,23] [,24] [,25] [,26] [,27] [,28] ## 5% 147.0674 148.4928 149.2981 150.1580 151.7772 151.4979 152.9998 ## 94% 163.1741 164.9025 165.5837 165.9044 166.7547 167.7817 168.9641 ## [,29] [,30] [,31] [,32] [,33] [,34] [,35] ## 5% 154.1103 154.4654 155.6601 156.6560 157.6031 158.3161 159.5117 ## 94% 169.7019 170.2732 171.7155 171.8713 173.9521 174.8402 175.4895 ## [,36] [,37] [,38] [,39] [,40] [,41] [,42] ## 5% 160.4609 160.9952 161.3552 162.1338 163.4090 164.5611 165.1676 ## 94% 176.2916 177.2782 178.1800 179.4627 180.3177 180.4383 181.5567 ## [,43] [,44] [,45] [,46] ## 5% 165.5014 167.6589 168.1717 168.8493 ## 94% 182.5953 183.3265 184.1630 185.3150 Now, height.PI contains the 89% (we can use any interval) posterior prediction interval of observable heights across the values of weights in weight.seq (i.e. the boundaries of the simulated heights the model expects) Lets plot everything: 1. the average line (MAP line) 2. shaded region of 89% plausible \\(\\mu\\) 3. boundaries of the simulated heights the model expects # plot data points plot(height ~ weight, d2, col=col.alpha(rangi2, 0.5)) # draw MAP line lines(weight.seq, mu.mean) # i used the border because the shade is not appearing for a bug related to R version shade(mu.PI, weight.seq,border=TRUE) shade(height.PI, weight.seq, border = TRUE) The narrow boundaries that are close to the line are the intervals of \\(\\mu\\). The wider boundary is the region within which the model expects to find 89% of actual heights in the population at each weight. The rouglness around the prediction interval is due to the simulation variance. We can decrease that by increasing the number of samples we take from the posterior. sim.height &lt;- sim(m4.3, data=list(weight=weight.seq), n=1e4) height.PI &lt;- apply(sim.height, 2, PI, prob=0.89) # plot data points plot(height ~ weight, d2, col=col.alpha(rangi2, 0.5)) # draw MAP line lines(weight.seq, mu.mean) # i used the border because the shade is not appearing for a bug related to R version shade(mu.PI, weight.seq,border=TRUE) shade(height.PI, weight.seq, border = TRUE) Curves from lines We can build models to describe the outcome as a curved function of a predictor using the linear regression. Here are the common methods: 1. Polynomial regression 2. B-Splines Polynomial regression The following data is seen to be followed a curved relationship library(rethinking) data(&quot;Howell1&quot;) d &lt;- Howell1 plot(height~weight, d) We can use the parabolic equation for representing the mean height: \\(\\mu_i = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2\\) The last parameter \\(\\beta_2\\) measures the curvature of the relationship Because the polynomial equations involve computing the square or curve of large number, we need to standarize the predictor values in order to avoid the errors in computing estimates. To standarize weight values we do the following: \\[ x_{std.} = \\frac{x - \\mu_x}{\\sigma_x} \\] This unit is called z-score. However, we will use \\(x\\) instead of \\(x_{std.}\\) in the following sections. This is the definition of our model: \\(h_i \\sim Normal(\\mu_i, \\sigma)\\) \\(\\mu_i = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2\\) \\(\\alpha \\sim Normal(178, 20)\\) \\(\\beta_1 \\sim Log-Normal(0, 1)\\) \\(\\beta_2 \\sim Normal(0, 1)\\) \\(\\sigma \\sim Uniform(0, 50)\\) Note that it is okay to have negative values for \\(\\beta_2\\). Lets code that and fit the model to our data: weight_s &lt;- (d$weight - mean(d$weight)) / sd(d$weight) weight_s2 &lt;- weight_s ^ 2 m4.5 &lt;- quap( alist( height ~ dnorm(mu, sigma), mu &lt;- a + b1*weight_s + b2*weight_s2, a ~ dnorm(178, 20), b1 ~ dlnorm(0, 1), b2 ~ dnorm(0, 1), sigma ~ dunif(0, 50) ), data=d ) precis(m4.5) ## mean sd 5.5% 94.5% ## a 146.055966 0.3690246 145.466193 146.645739 ## b1 21.733838 0.2889247 21.272081 22.195596 ## b2 -7.802658 0.2742178 -8.240911 -7.364405 ## sigma 5.775142 0.1765168 5.493034 6.057251 Lets summarize the prediction and plot it: weight.seq &lt;- seq(from=-2.2, to=2, length.out=30) pred_dat &lt;- list(weight_s=weight.seq, weight_s2=weight.seq^2) # compute predictions of mu for pred_dat as input mu &lt;- link(m4.5, data=pred_dat) mu.mean &lt;- apply(mu, 2, mean) mu.PI &lt;- apply(mu, 2, PI, prob=0.89) # simulate height values sim.height &lt;- sim(m4.5, data=pred_dat) height.PI &lt;- apply(sim.height, 2, PI, prob=0.89) plot(height ~ weight_s, d, col=col.alpha(rangi2, 0.5)) lines(weight.seq, mu.mean) shade(mu.PI, weight.seq) shade(height.PI, weight.seq) Remember that we are now working on the full data with both adults and non-adults, and thats why the relationship is not linear as it was with the adults data. Lets try building cubic regression on weight: \\(h_i \\sim Normal(\\mu_i, \\sigma)\\) \\(\\mu_i = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3\\) \\(\\alpha \\sim Normal(178, 20)\\) \\(\\beta_1 \\sim Log-Normal(0, 1)\\) \\(\\beta_2 \\sim Normal(0, 1)\\) \\(\\beta_3 \\sim Normal(0, 1)\\) \\(\\sigma \\sim Uniform(0,50)\\) weight_s3 &lt;- weight_s ^ 3 m4.6 &lt;- quap( alist( height ~ dnorm(mu, sigma), mu &lt;- a + b1*weight_s + b2*weight_s2 + b3*weight_s3, a ~ dnorm(178, 20), b1 ~ dlnorm(0, 1), b2 ~ dnorm(0, 1), b3 ~ dnorm(0, 1), sigma ~ dunif(0, 50) ), data=d ) precis(m4.6) ## mean sd 5.5% 94.5% ## a 146.394541 0.3099872 145.899122 146.889960 ## b1 15.219737 0.4762654 14.458573 15.980901 ## b2 -6.202618 0.2571582 -6.613606 -5.791629 ## b3 3.583373 0.2287735 3.217749 3.948997 ## sigma 4.829890 0.1469427 4.595047 5.064732 weight.seq &lt;- seq(from=-2.2, to=2, length.out=30) pred_dat_m4.6 &lt;- list(weight_s=weight.seq, weight_s2=weight.seq^2, weight_s3=weight.seq^3) mu &lt;- link(m4.6, data=pred_dat_m4.6) mu.mean &lt;- apply(mu, 2, mean) mu.PI &lt;- apply(mu, 2, PI, prob=0.89) sim.height &lt;- sim(m4.6, pred_dat_m4.6) height.PI &lt;- apply(sim.height, 2, PI, prob=0.89) plot(height ~ weight_s, d, col=col.alpha(rangi2, 0.5)) lines(weight.seq, mu.mean) shade(mu.PI, weight.seq) shade(height.PI, weight.seq) The cubic model is more flexible than others and thats why it fits well. However, we stoll have these issues in our model: Having a better fit \\(\\neq\\) Having a better model All the models we built so far have no biological information. We havent learnt any causal relationship so far The models are good geocentric model = meaning they describe the sample well Note that the x-axis contains the standardized weight values. To convet back to natural scale, we need to remove the current axis and build the axis explicitly: plot(height ~ weight_s, d, col=col.alpha(rangi2, 0.5), xaxt=&quot;n&quot;) lines(weight.seq, mu.mean) shade(mu.PI, weight.seq) shade(height.PI, weight.seq) at &lt;- c(-2,-1,0,1,2) # convert z-scores to weight values labels &lt;- at*sd(d$weight) + mean(d$weight) axis(side=1, at=at, labels=round(labels, 1)) Splines B-Spline stands for basis spline. It means that we can build wiggly functions from simple less-wiggly bassis components, that are basis functions We will use data of a 1000 years of blossoms days library(rethinking) data(&quot;cherry_blossoms&quot;) d &lt;- cherry_blossoms precis(d, hist=FALSE) ## mean sd 5.5% 94.5% ## year 1408.000000 350.8845964 867.77000 1948.23000 ## doy 104.540508 6.4070362 94.43000 115.00000 ## temp 6.141886 0.6636479 5.15000 7.29470 ## temp_upper 7.185151 0.9929206 5.89765 8.90235 ## temp_lower 5.098941 0.8503496 3.78765 6.37000 The B-Spline model d2 &lt;- d[complete.cases(d),] num_knots = 15 knot_list &lt;- quantile(d2$year, probs=seq(0, 1, length.out=num_knots)) library(splines) # create B-spline basis matrix B &lt;- bs(d2$year, knots=knot_list[-c(1, num_knots)], # -c(1, num_knots) means exclude the 1st and last element degree=3, intercept=TRUE) # Create an empty plot with specified axes { plot(NULL, xlim=range(d2$year), ylim=c(0,1), xlab=&quot;year&quot;, ylab=&quot;basis&quot;, type=&quot;n&quot;) # Plot knots for (knot in knot_list) { # Add a vertical line for each knot abline(v = knot, col = &quot;red&quot;, lty = 2, lwd = 2) } # Plot each column in the basis matrix against year for (i in 1:ncol(B)) { # Add lines for each column lines(d2$year, B[, i]) } } Building the model with quap m4.7 &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + B %*% w, # matrix multiplication a ~ dnorm(100, 10), w ~ dnorm(0, 1), sigma ~ dexp(1) ), data=list(D=d2$doy, B=B), start=list(w=rep(0, ncol(B))) ) Lets look at the posterior means: precis(m4.7) ## 17 vector or matrix parameters hidden. Use depth=2 to show them. ## mean sd 5.5% 94.5% ## a 104.705268 0.3330644 104.172966 105.237569 ## sigma 6.074996 0.1541797 5.828587 6.321405 Lets plot the posterior predictions: post &lt;- extract.samples(m4.7) # find the mean of all weights w &lt;- apply(post$w, 2, mean) plot(NULL, xlim=range(d2$year), ylim=c(-4,4), xlab=&quot;year&quot;, ylab=&quot;basis * weight&quot;) # plot the basis * weight for each column for (i in 1:ncol(B)) lines(d2$year, w[i]*B[,i]) # plot knots for (knot in knot_list) { abline(v = knot, col = &quot;red&quot;, lty = 2, lwd = 2) } # 97% posterior interval for mu at each year mu &lt;- link(m4.7) mu.PI &lt;- apply(mu, 2, PI, 0.97) plot(d2$year, d2$doy, col=col.alpha(rangi2, 0.3), pch=16) shade(mu.PI, d2$year, col=col.alpha(&quot;black&quot;, 0.5)) abline(h = mean(d2$doy, col=&quot;black&quot;)) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
